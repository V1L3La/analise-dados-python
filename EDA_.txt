                             PORTIF√ìLIO de INSIGHTS  (ANALISTA DE PROBLEMA/DOR)
                     fonte: https://chat.qwen.ai/c/3b3ccd3c-53e4-4e8f-b2ce-199473b44ed3 
================================================================
üåê 1. FONTE DE DADOS 

KAGGLE:  https://www.kaggle.com/datasets/ahmedmohamed2003/cafe-sales-dirty-data-for-cleaning-training/data

DESCRI√á√ÉO DA BASE: 
Um conjunto de dados simulado com transa√ß√µes de vendas em caf√©s, contendo informa√ß√µes sobre itens vendidos, quantidades, pre√ßos, m√©todos de pagamento, localiza√ß√£o e datas.
================================================================
üìã 2.COLUNAS DA BASE DE DADOS:

Item -                   O nome do item comprado.
Quantidade -             A quantidade do item comprado.
Pre√ßo por unidade -      O pre√ßo de uma √∫nica unidade do item.
Total gasto -            O valor total gasto na transa√ß√£o.
M√©todo de pagamento -    O m√©todo de pagamento utilizado.
Localiza√ß√£o-             O local onde a transa√ß√£o ocorreu.
Data da transa√ß√£o -      A data da transa√ß√£o.
================================================================
‚ö†Ô∏è 3. Problema / Dor Identificada (An√°lise de Neg√≥cio)

‚ùì Qual √© o problema real?                (RESOLVER ESSA DOR OU PROBLEMA)
   "As vendas est√£o caindo em determinadas lojas, mas n√£o sabemos por qu√™. 
    H√° suspeita de que o pre√ßo dos itens esteja desalinhado com o mercado, 
    ou que certos produtos n√£o estejam sendo bem-sucedidos." 

üîç Causa Raiz (HIP√ìTESES INICIAIS)       (ADAPTAR ESSAS HIP√ìTESES RELACIONADO AO PROBLEMA PROPOSTO)
   (Explora√ß√£o de dAdos EXPLANAT√ìRIA para investigar)
Com base no entendimento do neg√≥cio, levantei as seguintes hip√≥teses:
Ex:
‚û§ Produtos com pre√ßo alto t√™m baixa quantidade vendida.
‚û§ Alguns locais t√™m desempenho muito abaixo da m√©dia.
‚û§ M√©todos de pagamento mais populares s√£o subutilizados (ex: Pix).
‚û§ Existem inconsist√™ncias nos dados (ex: Total gasto ‚â† Quantidade √ó Pre√ßo por unidade) 
   que afetam a confiabilidade das an√°lises.
         
         ‚úÖ Objetivo: Validar essas hip√≥teses com an√°lise explorat√≥ria e 
                       limpeza de dados ‚Üí gerar insights acion√°veis. 
================================================================
üõ†Ô∏è 4.Etapas do Projeto (Pipeline de An√°lise de Dados)

1. Entendimento do Neg√≥cio
Compreens√£o do modelo de neg√≥cio:
  Ex: caf√© com foco em vendas di√°rias, m√∫ltiplas localiza√ß√µes, mix de produtos.

Defini√ß√£o de KPIs-chave: 
  Ex: volume de vendas, receita total, produtos mais vendidos, efici√™ncia por local.

2. Coleta e Limpeza de Dados (Data Wrangling)
Ex:
  ‚û§ Verifica√ß√£o de valores ausentes (NaN) em colunas cr√≠ticas.
  ‚û§ Corre√ß√£o de tipos de dados (ex: Data da transa√ß√£o como string ‚Üí datetime).
  ‚û§ Padroniza√ß√£o de nomes de produtos e m√©todos de pagamento.
  ‚û§ Detec√ß√£o e corre√ß√£o de inconsist√™ncias no Total gasto (compara√ß√£o com Quantidade √ó Pre√ßo por unidade).
  ‚û§ Remo√ß√£o de duplicatas e outliers extremos.

3. Explora√ß√£o de Dados (EDA ‚Äì Exploratory Data Analysis)
  ‚û§ Distribui√ß√£o de vendas por dia/semana/m√™s.
  ‚û§ Top 10 produtos mais vendidos (por quantidade e receita).
  ‚û§ Receita m√©dia por localiza√ß√£o e m√©todo de pagamento.
  ‚û§ Rela√ß√£o entre pre√ßo unit√°rio e quantidade vendida.
  ‚û§ An√°lise temporal: sazonalidade de vendas?

4. Valida√ß√£o de Hip√≥teses
‚û§ Produtos caros t√™m baixa demanda           ‚úÖ Verdadeiro: "Espresso" tem alto pre√ßo e baixa quantidade vendida
‚û§ Lojas em "Shopping" t√™m melhor desempenho  ‚úÖ Verdadeiro: Receita m√©dia +40% em compara√ß√£o com "Centro"
‚û§ Pix √© pouco usado                          ‚ùå Falso: Pix representa 58% das transa√ß√µes ‚Äì o mais utilizado

5. Gera√ß√£o de Insights(revela√ß√£o) & Propostas de Solu√ß√£o  
Ex:
‚ú® Insight 1: O produto "Cappuccino Premium" tem alto pre√ßo (US$ 6,50) e baixa quantidade vendida (m√©dia de 1,2/unidade).
‚û§ Recomenda√ß√£o: Reduzir o pre√ßo em 15% ou criar um combo com croissant para aumentar atratividade. 
‚ú® Insight 2: A loja no "Shopping" tem alta receita, mas poucas transa√ß√µes di√°rias.
‚û§ Recomenda√ß√£o: Campanha de marketing para atrair clientes no per√≠odo noturno (ex: happy hour). 
‚ú® Insight 3: O m√©todo de pagamento "Dinheiro" √© usado em apenas 8% das transa√ß√µes, apesar de ser o preferido por idosos.
‚û§ Recomenda√ß√£o: Oferecer cupom de desconto para quem pagar em dinheiro (incentivo ao uso). 
================================================================
üéØ 5. Conclus√£o e Valor Agregado

Este projeto demonstra minha capacidade de:
‚û§ Transformar dados sujos em informa√ß√µes valiosas.
‚û§ Usar Python no Databricks para an√°lise escal√°vel.
‚û§ Criar um fluxo completo: desde a limpeza at√© a tomada de decis√£o com base em evid√™ncias.
‚û§ Traduzir dados em insights acion√°veis para o neg√≥cio.
         üèÅ Resultado final: Uma proposta de melhoria no mix de produtos, estrat√©gias de precifica√ß√£o 
                             e campanhas promocionais baseadas em dados reais. 

üìé Anexos (Sugest√µes para o portf√≥lio)
  ‚û§ Notebook Databricks (link p√∫blico ou exportado)
  ‚û§ Gr√°ficos gerados (seguindo boas pr√°ticas de visualiza√ß√£o)
  ‚û§ Relat√≥rio resumido em PDF
  ‚û§ Apresenta√ß√£o (PowerPoint ou Canva) para mostrar o projeto para entrevistas 

‚úâÔ∏è Como apresentar isso?
Use esse conte√∫do como base para:
  ‚û§ Um GitHub com o c√≥digo (notebooks Databricks + README explicativo).
  ‚û§ Um LinkedIn post mostrando o processo e os resultados.
  ‚û§ Uma apresenta√ß√£o t√©cnica em entrevistas para mostrar sua jornada anal√≠tica.                            
================================================================





readme formata: https://towardsdatascience.com/cheat-sheet-for-google-colab-63853778c093/
                              http://cursos.leg.ufpr.br/prr/capMarkdown.html
Obs: cada c√©lula um coment√°rio
==================================


==================================
nomenclatura d:      df_nome_tratamento    OU nome abreviado
==================================

>>>>>>>>>>> ENTENDENDO A BASE  
knowledge:  https://github.com/V1L3La/Learning-An-lise-de-Dados-em-Python/blob/main/C%C3%B3pia_de_knowledge.ipynb

Carregar os dados
.info()

.head()         .head().T
.sample()
.tail()

.shape       igm[['municipio']].shape
.dtypes

 .describe()
        Converter tipos:  df['price'] = df['price'].astype(int)
 .describe(include='object')      informa√ß√µes categ√≥ricas(texto)
       Top=mais frequente    Freq=quantas vezes o top est√° repetindo

(no dataframe)
  .isnull().sum()     retorna o n√∫mero de valores ausentes para cada coluna.
  .isnull().sum().sum()    retorna o n√∫mero total de valores ausentes
  .isna().mean()    porcentagem de valores que est√£o faltando
  .fillna(value=10)  substituir valores de NaN por um escalar

(Na coluna)
.columns
.unique()      Quais valores √∫nicos numa coluna
.nunique()    quantos valores √∫nicos existem numa coluna
df[df.isnull().any(axis=1)]      quais linhas est√£o como NaN (dados ausentes): 
                                                   Tratar com: (Categ√≥ricos) -  moda, "N√£o informado","Desconhecido"
                                                   (Cont√≠nuos) -      m√©dia, mediana(P outliers), 0
df['nome']     Ver uma coluna
igm[['municipio', 'indice_governanca']]       Ver v√°rias colunas
df['regiao'].value_counts()
valores unicos  em cada categoria dentro de cada coluna
Renomeando o nome colunas do DataFrame
Renomeando o nome das categorias dentro de cada coluna do DataFrame

Visualiza√ß√£o:
sns.swarmplot(x="regiao", y="indice_governanca", hue='porte', data=igm)  


>>>>>>>>>>> TRATAMENTO

2- Limpeza de dados(Data cleaning)
"N√£o se apresse nessa fase"

-Duplicados
-Ausentes[preencher{para categ√≥ricos rotular como "ausente"}{para num√©ricos rotular como "0"} ou eliminar]
-Em branco
-Espa√ßo em branco
-Convers√£o
-Outliers indesejados[com valores discrepantes],
-Padronizar[Ex. DOG, CAT, Cachorro, Gato, cachorro, gato] [erros de digita√ß√£o]
-Irrelevantes[N√£o se usa]

Objetivos: Integridade dos dados, Exatid√£o dos dados, Precis√£o dos dados e Relev√¢ncia dos dados.







